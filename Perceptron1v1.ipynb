{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron1v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPK/GXrsEe7U2gtJTTMBfNd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wlwaters/Deep-Learning/blob/main/Perceptron1v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3Y_mYb1u6918"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ayvqs3WbYod",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "outputId": "a484043e-4647-4de9-ed44-183c910ba117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_set_x_flatten shape: (12288, 209)\n",
            "train_set_y shape: (1, 209)\n",
            "test_set_x_flatten shape: (12288, 50)\n",
            "test_set_y  shape: (1, 50)\n",
            "sigmoid(0) =0.5\n",
            "sigmoid(9.2) = 0.9998989708060922\n",
            "Loss after iteration 0: 0.297788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after iteration 100: 0.172249\n",
            "Loss after iteration 200: 0.172249\n",
            "Loss after iteration 300: 0.172249\n",
            "Loss after iteration 400: 0.172249\n",
            "Loss after iteration 500: 0.172249\n",
            "Loss after iteration 600: 0.172249\n",
            "Loss after iteration 700: 0.172249\n",
            "Loss after iteration 800: 0.172249\n",
            "Loss after iteration 900: 0.172249\n",
            "Loss after iteration 1000: 0.172249\n",
            "Loss after iteration 1100: 0.172249\n",
            "Loss after iteration 1200: 0.172249\n",
            "Loss after iteration 1300: 0.172249\n",
            "Loss after iteration 1400: 0.172249\n",
            "Loss after iteration 1500: 0.172249\n",
            "Loss after iteration 1600: 0.172249\n",
            "Loss after iteration 1700: 0.172249\n",
            "Loss after iteration 1800: 0.172249\n",
            "Loss after iteration 1900: 0.172249\n",
            "train accuracy: 65.55023923444976 %\n",
            "test accuracy: 34.0 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#\n",
        "#      EE 6183-Y01 Deep Learning  Homework #1  Group 1 Activation and Loss\n",
        "#       Import packages\n",
        "import h5py\n",
        "import scipy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "#\n",
        "# load_datasets\n",
        "#\n",
        "train_dataset = h5py.File('train_catvnoncat.h5',\"r\")\n",
        "train_set_x_orig=np.array(train_dataset[\"train_set_x\"][:])\n",
        "train_set_y =np.array(train_dataset[\"train_set_y\"][:])\n",
        "test_dataset = h5py.File('test_catvnoncat.h5',\"r\")\n",
        "test_set_x_orig=np.array(test_dataset[\"test_set_x\"][:])\n",
        "test_set_y =np.array(test_dataset[\"test_set_y\"][:])\n",
        "\n",
        "classes=np.array(test_dataset[\"list_classes\"][:])\n",
        "\n",
        "train_set_y =train_set_y.reshape((1, train_set_y.shape[0]))\n",
        "test_set_y =test_set_y.reshape((1, test_set_y.shape[0]))\n",
        "# \n",
        "# \n",
        "# Display dataset to check accessibility\n",
        "# \n",
        "\n",
        "#index = 25\n",
        "\n",
        "#plt.imshow(train_set_x_orig[index])\n",
        "#print (\"y =\" + str(train_set_y[:,index]) + \", it's a ' \" + \n",
        "#classes [np.squeeze(train_set_y[:,index])].decode(\"utf-8\") + \"'picture.\")\n",
        "\n",
        "\n",
        "m_train = train_set_y.shape[1]\n",
        "m_test = test_set_y.shape[1]\n",
        "num_px = train_set_x_orig.shape[1]\n",
        "\n",
        "\n",
        "# We must reshape the training and test datasets into single vectors for input \n",
        "\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],\n",
        "-1).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], \n",
        "-1).T\n",
        "print(\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "print(\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print(\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "print(\"test_set_y  shape: \" + str(test_set_y.shape))\n",
        "\n",
        "# Normalize the data by divide each row of data set by 255\n",
        "\n",
        "train_set_x = train_set_x_flatten / 255\n",
        "test_set_x = test_set_x_flatten / 255\n",
        "#X = train_set_x\n",
        "#Y = train_set_y\n",
        "#X_Test = test_set_x\n",
        "# ---------------------------------------------------------------------------\n",
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "   Compute the sigmoid of z\n",
        "   Arguments: x -- A scalar od numpy array of any size\n",
        "   Return:\n",
        "   S -- sigmoid(z)\n",
        "   \"\"\"\n",
        "  s = 1 / (1 + np.exp(-z))\n",
        "  return s\n",
        "\n",
        "\n",
        "print(\"sigmoid(0) =\" + str(sigmoid(0)))\n",
        "print(\"sigmoid(9.2) = \" + str(sigmoid(9.2)))\n",
        "# -------------------------------------------------------------------------\n",
        "# dim -- size of the w vector we want\n",
        "# w -- initialized vector of shape (dim, 1)\n",
        "# b -- initialized scalar (corresponds to the bias)\n",
        "# Function creates a vector of zeroes of shape (dim, 1) for w and set b to 1\n",
        "# -------------------------------------------------------------------------\n",
        "def initialize_with_zeros(dim):\n",
        "  w = np.zeros(shape=(dim,1))\n",
        "  b = 1\n",
        "  return w,b\n",
        "# --------------------------------------------------------------------------\n",
        "# dim = 2\n",
        "# w, b = initialize_with_zeros(dim)\n",
        "# print(\"w = \" + str(w))\n",
        "# print(\"b = \" + str(b))\n",
        "\n",
        "def propagate(w,b,X,Y):\n",
        "  # Implement the loss function and its gradient for the forward and \n",
        "  # backward propagation\n",
        "  m = X.shape[1]\n",
        "  train_set_x=np.array(X)*5\n",
        "  # FORWARD PROPAGATION (GROUP 1)\n",
        "  # Activation function for Group 1 based on given function\n",
        "  # ---------------------------------------------------------\n",
        "  A = sigmoid(np.dot(w.T,train_set_x) + (3*b))\n",
        "  loss = (1.0/(2*m)) * np.sum((Y-A)*(Y-A))\n",
        "  # ---------------------------------------------------------\n",
        "  # BACKWARD PROPAGATION (TO FIND GRADS)\n",
        "  Z1 = np.dot(A, (1-A).T)\n",
        "  Z2 = np.dot(X, (Y-A).T)\n",
        "  dw = (-1.0 / m) * (Z1*Z2)\n",
        "  db = (-1.0 / m) * np.sum(Z1*(Y-A))\n",
        "  assert(dw.shape == w.shape)\n",
        "  assert(db.dtype == float)\n",
        "  loss = np.squeeze(loss)\n",
        "  assert(loss.shape == ())\n",
        "\n",
        "  grads = {\"dw\": dw, \"db\": db}\n",
        "\n",
        "  return grads, loss\n",
        "#\n",
        "# -------------------------------------------------------------------------\n",
        "#  OPTIMIZE USING GRADIENT DESCENT\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "                                                                             \n",
        "                                                                             \n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_loss = False):\n",
        "  # This function optimizes w and b by running a gradient_descent algorithm\n",
        "  # w -- weights, a nump array of size (num_px * num_px *)\n",
        "  # b -- bias. a scalar\n",
        "  # X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "  # Y -- true \"label\" vector (0 if non-cat, 1 if cat)\n",
        "  # num_iterations \n",
        "  # learning_rate -- learning rate of the gradient descent update rule\n",
        "  # print_loss -- True means to print the loss every 100 steps\n",
        "  #\n",
        "  # Returns\n",
        "  # params -- dictionary containing the weiths w and bias b \n",
        "  # grads -- dictionary containing the gradients of the weights and bias with\n",
        "  #          respect to the loss function\n",
        "  # Loss -- List of all the loss computed during the optimization, this will\n",
        "  #         be used to plot the learning curve\n",
        "  losses = []\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "    # Loss and gradient calculation (~ 1-4 lines of code)\n",
        "    grads, loss = propagate(w, b, X, Y)\n",
        "    # Retrieve derivatives from grads\n",
        "    dw = grads[\"dw\"]\n",
        "    db = grads[\"db\"]\n",
        "    # update rule (~ 2 lines of code)\n",
        "    w = w - learning_rate * dw \n",
        "    b = b -learning_rate * db\n",
        "    # Record the losses\n",
        "    if i % 100 == 0:\n",
        "      losses.append(loss)\n",
        "    # Print the loss every 100 training examples\n",
        "    if print_loss and i % 100 == 0:\n",
        "      print(\"Loss after iteration %i: %f\" % (i, loss))\n",
        "  params = {\"w\":w, \"b\": b}\n",
        "  grads = {\"dw\": dw, \"db\": db}\n",
        "\n",
        "  return params, grads, losses\n",
        "#\n",
        "#\n",
        "# Predict --------------------------------------------------------------------\n",
        "#\n",
        "def predict(w, b, X):\n",
        "  # Predict whether the label is 0 or 1 using learned Logistics \n",
        "  # regression parameters (w,b)\n",
        "  # \n",
        "  # Returns: \n",
        "  #    Y_prediction -- a numpy array (vector) containing all predictiions\n",
        "  #                    (0/1) for the examples in X\n",
        "  #\n",
        "  m = X.shape[1]\n",
        "  Y_prediction = np.zeros((1,m))\n",
        "  w = w.reshape(X.shape[0],1)\n",
        "  # Compute vector \"A\" predicting the probabilities of a cat being present\n",
        "  # in the picture \n",
        "  #\n",
        "  A = sigmoid(np.dot(w.T,X) + b)\n",
        "  for i in range(A.shape[1]):\n",
        "    # Convert probabilities a [0,i] to actual predictions p[0,1]\n",
        "    Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
        "\n",
        "    assert(Y_prediction.shape == (1,m))\n",
        "\n",
        "    return Y_prediction\n",
        "\n",
        "### -------------------------------------------------------------------------\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate =\n",
        ".01, print_loss=False):\n",
        "#\n",
        "# initialize parameters with zeros ()\n",
        "#\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "# Gradient descent \n",
        "    params, grads, losses = optimize(w,b, X_train, Y_train,num_iterations,\n",
        "    learning_rate, print_loss)\n",
        "    w = params[\"w\"]\n",
        "    b = params[\"b\"]\n",
        "# Predict test/test/train set examples\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "#\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train\n",
        "    - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100-np.mean(np.abs(Y_prediction_test\n",
        "    - Y_test )) * 100))\n",
        "\n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\" : Y_prediction_train,\n",
        "         \"w\" : w,\n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    return d\n",
        "# --------------------------------------------------------------------------\n",
        "# Run the model \n",
        "# --------------------------------------------------------------------------\n",
        "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = \n",
        "          2000, learning_rate=.01, print_loss = True)\n",
        "# --------------------------------------------------------------------------\n",
        "# Plot the Results\n",
        "losses = np.squeeze(d['losses'])\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss - Group 1')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GB1HhvnS_Gyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PP-BAJhU4_7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WZffmfKD5EY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mCHVivVBmHlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C5UJv5bBlvGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8H0b5U2Hlv2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "KTblfELzj348"
      }
    }
  ]
}
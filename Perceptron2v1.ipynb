{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron2v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTPk2Ci1nZiNg20/8H72RG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wlwaters/Deep-Learning/blob/main/Perceptron2v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3Y_mYb1u6918"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "53y8XIhLTBOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ayvqs3WbYod",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "b22bb089-358e-46f8-edea-e7c62b93e9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after iteration 0: 0.375248\n",
            "Loss after iteration 100: 0.344498\n",
            "Loss after iteration 200: 0.344498\n",
            "Loss after iteration 300: 0.344498\n",
            "Loss after iteration 400: 0.344498\n",
            "Loss after iteration 500: 0.344498\n",
            "Loss after iteration 600: 0.344498\n",
            "Loss after iteration 700: 0.344498\n",
            "Loss after iteration 800: 0.344498\n",
            "Loss after iteration 900: 0.344498\n",
            "Loss after iteration 1000: 0.344498\n",
            "Loss after iteration 1100: 0.344498\n",
            "Loss after iteration 1200: 0.344498\n",
            "Loss after iteration 1300: 0.344498\n",
            "Loss after iteration 1400: 0.344498\n",
            "Loss after iteration 1500: 0.344498\n",
            "Loss after iteration 1600: 0.344498\n",
            "Loss after iteration 1700: 0.344498\n",
            "Loss after iteration 1800: 0.344498\n",
            "Loss after iteration 1900: 0.344498\n",
            "train accuracy: 65.55023923444976 %\n",
            "test accuracy: 34.0 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfZRddX3v8fcn80AyJwTmDKMVAiZgqEVRChGxV5AWpEG8gi0CykWkD5hKhNbbCr0ipazapVjpkmsuXKyID8T4gJSowaC9RaSKMmB4CIIkESURISaREBLIJPneP/bvhJ3hzMw5mbPnzJzzea111pz927+9z3fvefjO3r+9v1sRgZmZWa2mNDsAMzObXJw4zMysLk4cZmZWFycOMzOrixOHmZnVxYnDzMzq4sRhVoWkYyU90uw4zCYiJw6bcCQ9JunEZsYQEd+PiN9tZgwVko6XtGacPusESQ9L2iLpPyW9fIS+s1KfLWmZE3PzXi1pmaTfSPLNYi3GicPakqSOZscAoMyE+D2UtB/wdeDDQBkYAL48wiJfAn4C9AEfAr4mqT/NGwS+Avx5YQFb00yIH1izWkiaIukSSaskrZf0FUnl3PyvSvq1pKcl3SHpVbl5N0i6RtJSSc8Cf5iObP5W0v1pmS9Lmpr67/Zf/kh90/wPSnpC0q8k/YWkkPSKYbbjdkkfkfRfwBbgYEnnSfqppGckrZb03tS3BNwK7C9pc3rtP9q+2EN/AqyIiK9GxHPA5cBrJb2yyjYcChwJ/ENEbI2Im4AHgD8FiIhHIuIzwIoxxmQTkBOHTSbvB04D3gTsD2wEFubm3wrMAV4C3AvcOGT5dwEfAfYG7kxtZwDzgNnAa4D3jPD5VftKmgd8ADgReAVwfA3bcg5wforlF8BTwFuBGcB5wL9KOjIingVOBn4VEdPT61c17ItdJB0k6bcjvN6Vur4KuK+yXPrsVal9qFcBqyPimVzbfcP0tRbT2ewAzOowH1gQEWsAJF0O/FLSORGxPSKur3RM8zZK2icink7Nt0TEf6X3z0kCuDr9IUbSN4AjRvj84fqeAXw2IlbkPvvsUbblhkr/5Fu599+TdBtwLFkCrGbEfZHvGBG/BPYdJR6A6cC6IW1PkyW3an2frtL3gBo+xyY5H3HYZPJy4ObKf8rAT4EdwEsldUj6aDp1swl4LC2zX275x6us89e591vI/iAOZ7i++w9Zd7XPGWq3PpJOlnSXpA1p297C7rEPNey+qOGzh7OZ7IgnbwbwzBj7Wotx4rDJ5HHg5IjYN/eaGhFryU5DnUp2umgfYFZaRrnli7q65wlgZm76wBqW2RWLpL2Am4B/AV4aEfsCS3kh9mpxj7QvdpNOVW0e4VU5OloBvDa3XAk4hOrjFCvIxmbyRyOvHaavtRgnDpuouiRNzb06gWuBj1QuEZXUL+nU1H9v4HlgPdAD/PM4xvoV4DxJvyeph+yqpHp0A3uRnSbaLulk4KTc/CeBPkn75NpG2he7iYhf5sZHqr0qY0E3A6+W9Kdp4P8y4P6IeLjKOn8GLAf+IX1/3k427nNTikdpHd1pempKkNYCnDhsoloKbM29Lgc+CSwBbpP0DHAX8PrU//Nkg8xrgYfSvHEREbcCVwP/CazMffbzNS7/DHAhWQLaSHb0tCQ3/2GyS19Xp1NT+zPyvtjT7VhHdlXUR1IcrwfOqsyXdK2ka3OLnAXMTX0/Cpye1gHZqbStvHAEshXwDZUtQn6Qk1ljSfo94EFgr6ED1WatwEccZg0g6e2S9pLUC3wM+IaThrUqJw6zxngv2b0Yq8iubvqr5oZjVhyfqjIzs7r4iMPMzOrSFneO77fffjFr1qxmh2FmNqncc889v4mI/qHtbZE4Zs2axcDAQLPDMDObVCT9olq7T1WZmVldnDjMzKwuThxmZlYXJw4zM6uLE4eZmdXFicPMzOrixGFmZnUpNHFImifpEUkrJV1SZf58SQ9IWi7pTkmHpfazU1vltVPSEWne7WmdlXkvKSr+z/3gMZbc96uiVm9mNikVljgkdQALgZOBw4B3VhJDzqKIODwijgCuBK4CiIgbI+KI1H4O8POIWJ5b7uzK/Ih4qqht+NKPf8mS5U4cZmZ5RR5xHA2sjIjVEbENWEz2aM9dImJTbrJE9UdkvjMtO+7KpW42PFvTs3jMzNpGkYnjALLnIlesSW27kXSBpFVkRxwXVlnPmWRPP8v7bDpN9WFJqrIMks6XNCBpYN26ddW6jKpc6mbjlsE9WtbMrFU1fXA8IhZGxCHAxcCl+XmSXg9siYgHc81nR8ThwLHpdc4w670uIuZGxNz+/hfV6KpJX6mb9Zt9xGFmlldk4lgLHJibnpnahrMYOG1I21kMOdqIiLXp6zPAIrJTYoUol/Zi03PbGdyxs6iPMDObdIpMHHcDcyTNltRNlgSW5DtImpObPAV4NDdvCnAGufENSZ2S9kvvu4C3kj3buRDlUhcAG5/dVtRHmJlNOoWVVY+I7ZIWAMuADuD6iFgh6QpgICKWAAsknQgMAhuBc3OrOA54PCJW59r2ApalpNEBfBf4dFHbUC7tBcCGLdt4yYypRX2MmdmkUujzOCJiKbB0SNtlufcXjbDs7cAxQ9qeBY5qbJTDK5e6Adiw2UccZmYVTR8cn8j6pmeJY71PVZmZ7eLEMYLennTE4cRhZraLE8cIenuywXEnDjOzFzhxjKCzYwr79nQ5cZiZ5ThxjCIrO+LEYWZW4cQxinJPN+tdr8rMbBcnjlGUS91sfNb1qszMKpw4RtE3vduX45qZ5ThxjCKrkLuNnTurVXw3M2s/Thyj6O3pZsfOYNNzPl1lZgZOHKOq3D3uK6vMzDJOHKPYVejQicPMDHDiGFVfyfWqzMzynDhG0VvyqSozszwnjlH0OXGYme3GiWMUU7s66OnucOIwM0ucOGrgelVmZi9w4qhBueS7x83MKpw4apDVq3LiMDMDJ46a+FSVmdkLnDhq0FdyaXUzswonjhr0lrp5bnAnW7Ztb3YoZmZN58RRA9/LYWb2gkITh6R5kh6RtFLSJVXmz5f0gKTlku6UdFhqPzu1VV47JR2R5h2Vllkp6WpJKnIbwPWqzMzyCksckjqAhcDJwGHAOyuJIWdRRBweEUcAVwJXAUTEjRFxRGo/B/h5RCxPy1wD/CUwJ73mFbUNFWXXqzIz26XII46jgZURsToitgGLgVPzHSJiU26yBFR7WtI707JIehkwIyLuiogAPg+cVkTweZXEsWGzE4eZWWeB6z4AeDw3vQZ4/dBOki4APgB0A39UZT1n8kLCOSCtJ7/OA6p9uKTzgfMBDjrooDpD310lcWzc4sRhZtb0wfGIWBgRhwAXA5fm50l6PbAlIh7cg/VeFxFzI2Juf3//mGKcMbWTrg75VJWZGcUmjrXAgbnpmaltOIt58Wmns4AvDVnnzDrW2RCS6O3p9qkqMzOKTRx3A3MkzZbUTZYEluQ7SJqTmzwFeDQ3bwpwBml8AyAingA2STomXU31buCW4jbhBa5XZWaWKWyMIyK2S1oALAM6gOsjYoWkK4CBiFgCLJB0IjAIbATOza3iOODxiFg9ZNXvA24ApgG3plfhyqVuj3GYmVHs4DgRsRRYOqTtstz7i0ZY9nbgmCrtA8CrGxdlbcqlblb8atPoHc3MWlzTB8cni75SN+s3u16VmZkTR416S91sem47gzt2NjsUM7OmcuKoUZ/v5TAzA5w4auZ6VWZmGSeOGrnsiJlZxomjRi50aGaWceKoketVmZllnDhq1NvTBcB6n6oyszbnxFGjzo4p7NvT5cFxM2t7Thx1KPd0O3GYWdtz4qhDueTEYWbmxFEHJw4zMyeOuvRNd2l1MzMnjjr09mSl1bPHnZuZtScnjjqUS93s2Bls2rq92aGYmTWNE0cd+qZX7h53eXUza19OHHVwoUMzMyeOupR7UqFDJw4za2NOHHUoT3fiMDNz4qhDnyvkmpk5cdRjalcHPd0dPuIws7bmxFGn3p5uNjpxmFkbc+Kok+8eN7N2V2jikDRP0iOSVkq6pMr8+ZIekLRc0p2SDsvNe42kH0pakfpMTe23p3UuT6+XFLkNQ7lelZm1u86iViypA1gIvBlYA9wtaUlEPJTrtigirk393wZcBcyT1Al8ETgnIu6T1AcM5pY7OyIGiop9JOVSN48+ubkZH21mNiEUecRxNLAyIlZHxDZgMXBqvkNEbMpNloBKEaiTgPsj4r7Ub31E7Cgw1pr5mRxm1u72KHFIml5DtwOAx3PTa1Lb0HVdIGkVcCVwYWo+FAhJyyTdK+mDQxb7bDpN9WFJGibG8yUNSBpYt25dDeHWpjy9m62DO9i6bULkMTOzcbenRxwPjd6lNhGxMCIOAS4GLk3NncAbgbPT17dLOiHNOzsiDgeOTa9zhlnvdRExNyLm9vf3Nyrc3L0crldlZu1p2DEOSR8YbhZQyxHHWuDA3PTM1DacxcA16f0a4I6I+E2KZSlwJPAfEbEWICKekbSI7JTY52uIpyHy9apm9vaM18eamU0YIx1x/DPQC+w95DV9lOUq7gbmSJotqRs4C1iS7yBpTm7yFODR9H4ZcLiknjRQ/ibgIUmdkvZLy3YBbwUerCGWhimXugCXHTGz9jXSVVX3Av8eEfcMnSHpL0ZbcURsl7SALAl0ANdHxApJVwADEbEEWCDpRLIrpjYC56ZlN0q6iiz5BLA0Ir4lqQQsS0mjA/gu8Ok6tnfMXCHXzNrdSInjPGD9MPPm1rLyiFgKLB3Sdlnu/UUjLPtFskty823PAkfV8tlFKZdc6NDM2tuwiSMiHhlh3pPFhDPxzZjaSecU+e5xM2tbLjlSJ0n0llyvyszalxPHHugruV6VmbUvJ4494HpVZtbORk0ckg6W9A1Jv5H0lKRbJB08HsFNVE4cZtbOajniWAR8BfgdYH/gq8CXigxqonPiMLN2Vkvi6ImIL0TE9vT6IjC16MAmsnKpm6e3DjK4Y2ezQzEzG3e1JI5bJV0iaZakl6eCg0sllSWViw5wIqrUq9q4xUcdZtZ+ankexxnp63uHtJ9Fdld324135O8ef8nebX3wZWZtaNTEERGzxyOQyaTX9arMrI2Nmjgkvbtae0SMW0XaiabP9arMrI3Vcqrqdbn3U4ETyAogtm3icL0qM2tntZyqen9+WtK+ZM/OaFu9PdmpqvWbnTjMrP3syZ3jzwJtPe7R2TGFfaZ1+aoqM2tLtYxxfIPs6inInoHxe2Q3BLY116sys3ZVyxjHv+Tebwd+ERFrCopn0iiXutngU1Vm1oZGPVUVEd8DHiZ7bGwv4L+WuOyImbWvWoocngH8GHgH2c2AP5J0etGBTXTlUjcbPMZhZm2ollNVHwJeFxFPAUjqJ3vW99eKDGyiK6eHOUUEkpodjpnZuKnlqqoplaSRrK9xuZZWLnWzfWewaev2ZodiZjauajni+LakZbxQSv1MYGlxIU0OfdOzmwDXP/s8+6T7OszM2sGIiUPZOZirye4ef2Nqvi4ibi46sImut8cVcs2sPY2YOCIiJC2NiMOBr49TTJNCpV6V7x43s3ZTy1jFvZJeN3q3F5M0T9IjklZKuqTK/PmSHpC0XNKdkg7LzXuNpB9KWpH6TE3tR6XplZKuVpNGpsvTXa/KzNpTLYnj9cAPJa2SdH/6o33/aAtJ6gAWAicDhwHvzCeGZFFEHB4RRwBXAlelZTuBLwLzI+JVwPHAYFrmGuAvgTnpNa+GbWi4ck9ljMOJw8zaSy2D43+8h+s+GlgZEasBJC0GTgUeqnSIiE25/iVeKG1yEnB/RNyX+q1P63gZMCMi7krTnwdOA27dwxj32LTuDqZ1dbDRicPM2kwtd47/IiJ+QVZuJNJrbQ3rPgB4PDe9JrXtRtIFklaRHXFcmJoPBULSMkn3psfVVtaZL3dSdZ1pvedLGpA0sG7duhrCrZ/vHjezdjRs4pD095IuyzX9EPgmcBvwd40KICIWRsQhwMXApam5k+wqrrPT17dLOqHO9V4XEXMjYm5/f3+jwt1N33QXOjSz9jPSEcc7gE/kptdHxGuAVwGn1LDutcCBuemZjHykspjstBNkRxJ3RMRvImIL2X0jR6blZ9axzkKVS92+HNfM2s6Ip6oi4tnc5CdT2w5gWg3rvhuYI2m2pG7gLGBJvoOkObnJU4BH0/tlwOGSetJA+ZuAhyLiCWCTpGPS1VTvBm6pIZZClHu6fTmumbWdkQbHp0vqiohBgIi4AUDSXsCM0VYcEdslLSBLAh3A9RGxQtIVwEBELAEWSDqR7IqpjcC5admNkq4iSz4BLI2Ib6VVvw+4gSx53UoTBsYrPMZhZu1opMTxNeD/SlqQThchqQR8ihoLHEbEUoaUJ4mIy3LvLxph2S+SXZI7tH0AeHUtn1+08vRutg7uYOu2HUzr7mh2OGZm42KkU1UfBp4CfinpHkn3AI8BT6Z5ba+vlG4C9DiHmbWRYY840ljGJZL+EXhFal4ZEVvHJbJJoFKvasPmbRywby3DPmZmk9+oNwCmRPHAOMQy6eQr5JqZtYu2f67GWJRToUMPkJtZO3HiGINyyYUOzaz91JU4JF1eUByT0oypnXROkROHmbWVeo843lZIFJOUJHp9L4eZtZl6E0dTnn0xkfWVXK/KzNpLvYnjqEKimMTKpW6XVjeztlJX4oiInUUFMln5VJWZtRtfVTVGPlVlZu3GiWOMyqVunt46yOAOH4yZWXsYNXFIukjSDGU+k57Id9J4BDcZVOpV/XbL4Cg9zcxaQy1HHH+Wng1+EtALnAN8tNCoJpFe3wRoZm2mlsRRuQT3LcAXImIFvix3l8rd465XZWbtopbEcY+k28gSxzJJewM+oZ/0uV6VmbWZUavjAn8OHAGsjogtksrAecWGNXlUjjh8L4eZtYtajjjeADwSEb+V9D+AS4Gniw1r8ti3pwvAl+SaWduoJXFcA2yR9FrgfwKrgM8XGtUk0tUxhX2mdflUlZm1jVoSx/aICOBU4FMRsRDYu9iwJhffBGhm7aSWMY5nJP092WW4x0qaAnQVG9bk4npVZtZOajniOBN4nux+jl8DM4GPFxrVJON6VWbWTkZNHClZ3AjsI+mtwHMR4TGOHJ+qMrN2UkvJkTOAHwPvAM4AfiTp9FpWLmmepEckrZR0SZX58yU9IGm5pDslHZbaZ0namtqXS7o2t8ztaZ2VeS+pdWOLUjlVlQ0FmZm1tlrGOD4EvC4ingKQ1A98F/jaSAtJ6gAWAm8G1gB3S1oSEQ/lui2KiGtT/7cBVwHz0rxVEXHEMKs/OyIGaoh9XJRL3WzfGWx6bjv7TPPwj5m1tlrGOKZUkkayvsbljgZWRsTqiNgGLCa7MmuXVAOrogRMyn/Zy65XZWZtpJYE8G1JyyS9R9J7gG8BS2tY7gDg8dz0mtS2G0kXSFoFXAlcmJs1W9JPJH1P0rFDFvtsOk31YUlV62ZJOl/SgKSBdevW1RDunnshcbhelZm1vloGx/8OuA54TXpdFxEXNyqAiFgYEYcAF5PdlQ7wBHBQRPw+8AFgkaQZad7ZEXE4cGx6nTPMeq+LiLkRMbe/v79R4VZVqVe1frOPOMys9dUyxkFE3ATcVOe61wIH5qZnprbhLCa7S52IeJ7sEmAi4p50RHIoMBARa1P7M5IWkZ0Sa+pVXuXpqV7VFicOM2t9wx5xSHpG0qYqr2ckbRpuuZy7gTmSZkvqBs4Clgz5jDm5yVOAR1N7fxpcR9LBwBxgtaROSful9i7grcCDtW9uMco9ldLqThxm1vqGPeKIiDGVFYmI7ZIWAMuADuD6iFgh6QqyI4clwAJJJwKDwEbg3LT4ccAVkgbJSrjPj4gNkkpkpd270jq/C3x6LHE2wrTuDqZ1dbDBp6rMrA3UdKpqT0XEUoYMpEfEZbn3Fw2zXNVTYxHxLHBUg8NsiLLvHjezNlHLVVVWg77p3WzwGIeZtQEnjgbp7fERh5m1ByeOBukrdftyXDNrC04cDeIxDjNrF04cDVKe3s3WwR1s3baj2aGYmRXKiaNBKvdyeIDczFqdE0eD7KpX5XEOM2txThwN0je9cve4Cx2aWWtz4miQcip06HpVZtbqnDgaZFe9Kp+qMrMW58TRIDOmddI5Rb4k18xanhNHg0ii1/dymFkbcOJooD4nDjNrA04cDeR6VWbWDpw4Gqg83YnDzFqfE0cD9ZVcWt3MWp8TRwOVS938dssg23fsbHYoZmaFceJooErZkY1bBpsciZlZcZw4GmhXvSqPc5hZC3PiaCAnDjNrB04cDdSX6lU5cZhZK3PiaKDeUhcAG1wh18xamBNHA/VWCh36iMPMWlihiUPSPEmPSFop6ZIq8+dLekDSckl3Sjostc+StDW1L5d0bW6Zo9IyKyVdLUlFbkM9ujqmsM+0LjY6cZhZCysscUjqABYCJwOHAe+sJIacRRFxeEQcAVwJXJWbtyoijkiv+bn2a4C/BOak17yitmFP9JW6fcRhZi2tyCOOo4GVEbE6IrYBi4FT8x0iYlNusgTESCuU9DJgRkTcFREBfB44rbFhj40r5JpZqysycRwAPJ6bXpPadiPpAkmryI44LszNmi3pJ5K+J+nY3DrXjLbOZio7cZhZi2v64HhELIyIQ4CLgUtT8xPAQRHx+8AHgEWSZtSzXknnSxqQNLBu3brGBj0Cl1Y3s1ZXZOJYCxyYm56Z2oazmHTaKSKej4j16f09wCrg0LT8zFrWGRHXRcTciJjb39+/xxtRr3Kpm41btpGdSTMzaz1FJo67gTmSZkvqBs4CluQ7SJqTmzwFeDS196fBdSQdTDYIvjoingA2STomXU31buCWArehbuVSN4M7gk3PbW92KGZmhegsasURsV3SAmAZ0AFcHxErJF0BDETEEmCBpBOBQWAjcG5a/DjgCkmDwE5gfkRsSPPeB9wATANuTa8JI192ZJ9pXU2Oxsys8QpLHAARsRRYOqTtstz7i4ZZ7ibgpmHmDQCvbmCYDZVPHLP3KzU5GjOzxmv64Hircb0qM2t1ThwN5npVZtbqnDgarHLE4bvHzaxVOXE02LTuDqZ1dbhelZm1LCeOApRdr8rMWpgTRwFcdsTMWpkTRwGcOMyslTlxFMD1qsyslTlxFMBHHGbWypw4CtBb6mbLth08N7ij2aGYmTWcE0cB+kp+9riZtS4njgJU6lX5Xg4za0VOHAXom+4jDjNrXU4cBejtqVTIdb0qM2s9ThwF2FWvarOPOMys9ThxFGDGtE46poiNW5w4zKz1OHEUQBK9Pb6Xw8xakxNHQfpK3T5VZWYtyYmjIL573MxalRNHQcrTu9ngMQ4za0FOHAUpe4zDzFqUE0dByqVufrtlkO07djY7FDOzhnLiKEjl7vGNWwabHImZWWM5cRRkV70qj3OYWYspNHFImifpEUkrJV1SZf58SQ9IWi7pTkmHDZl/kKTNkv421/ZYbpmBIuMfi3IqO+JLcs2s1XQWtWJJHcBC4M3AGuBuSUsi4qFct0URcW3q/zbgKmBebv5VwK1VVv+HEfGbYiJvjPL0Sr0qJw4zay1FHnEcDayMiNURsQ1YDJya7xARm3KTJSAqE5JOA34OrCgwxsJUTlW50KGZtZoiE8cBwOO56TWpbTeSLpC0CrgSuDC1TQcuBv6xynoDuE3SPZLOH+7DJZ0vaUDSwLp168awGXvmhQq5Hhw3s9bS9MHxiFgYEYeQJYpLU/PlwL9GxOYqi7wxIo4ETgYukHTcMOu9LiLmRsTc/v7+IkIfUVfHFGZM7fQRh5m1nMLGOIC1wIG56ZmpbTiLgWvS+9cDp0u6EtgX2CnpuYj4VESsBYiIpyTdTHZK7I6GR98AfdP34us/WcsPVq1vdihm1qa+eeEb2auzo6HrLDJx3A3MkTSbLGGcBbwr30HSnIh4NE2eAjwKEBHH5vpcDmyOiE9JKgFTIuKZ9P4k4IoCt2FM5r/pYL73s/E/TWZmViHU8HUWljgiYrukBcAyoAO4PiJWSLoCGIiIJcACSScCg8BG4NxRVvtS4GZJldgXRcS3i9qGsTrzdQdx5usOanYYZmYNpYgYvdckN3fu3BgYmLC3fJiZTUiS7omIuUPbmz44bmZmk4sTh5mZ1cWJw8zM6uLEYWZmdXHiMDOzujhxmJlZXZw4zMysLm1xH4ekdcAv9nDx/YCJXMLd8Y2N4xsbxzc2Ez2+l0fEi4r9tUXiGAtJA9VugJkoHN/YOL6xcXxjM9HjG45PVZmZWV2cOMzMrC5OHKO7rtkBjMLxjY3jGxvHNzYTPb6qPMZhZmZ18RGHmZnVxYnDzMzq4sSRSJon6RFJKyVdUmX+XpK+nOb/SNKscYztQEn/KekhSSskXVSlz/GSnpa0PL0uG6/40uc/JumB9NkveviJMlen/Xe/pCPHMbbfze2X5ZI2SfrrIX3Gdf9Jul7SU5IezLWVJX1H0qPpa+8wy56b+jwqabSHnzUyvo9Lejh9/26WtO8wy474s1BgfJdLWpv7Hr5lmGVH/F0vML4v52J7TNLyYZYtfP+NWUS0/YvsCYWrgIOBbuA+4LAhfd4HXJvenwV8eRzjexlwZHq/N/CzKvEdD3yzifvwMWC/Eea/BbgVEHAM8KMmfq9/TXZjU9P2H3AccCTwYK7tSuCS9P4S4GNVlisDq9PX3vS+d5ziOwnoTO8/Vi2+Wn4WCozvcuBva/j+j/i7XlR8Q+Z/ArisWftvrC8fcWSOBlZGxOqI2AYsBk4d0udU4HPp/deAE5SeYVu0iHgiIu5N758BfgocMB6f3UCnAp+PzF3AvpJe1oQ4TgBWRcSeVhJoiIi4A9gwpDn/M/Y54LQqi/4x8J2I2BARG4HvAPPGI76IuC0itqfJu4CZjf7cWg2z/2pRy+/6mI0UX/q7cQbwpUZ/7nhx4sgcADyem17Di/8w7+qTfnmeBvrGJbqcdIrs94EfVZn9Bkn3SbpV0qvGNTAI4DZJ90g6v8r8WvbxeDiL4X9hm7n/AF4aEU+k978GXlqlz0TZj39GdgRZzWg/C0VakE6lXT/Mqb6JsP+OBZ6MiEeHmd/M/VcTJ45JRNJ04CbgryNi05DZ95Kdfnkt8L+Bf8kA58sAAAbZSURBVB/n8N4YEUcCJwMXSDpunD9/VJK6gbcBX60yu9n7bzeRnbOYkNfKS/oQsB24cZguzfpZuAY4BDgCeILsdNBE9E5GPtqY8L9LThyZtcCBuemZqa1qH0mdwD7A+nGJLvvMLrKkcWNEfH3o/IjYFBGb0/ulQJek/cYrvohYm74+BdxMdkogr5Z9XLSTgXsj4smhM5q9/5InK6fv0tenqvRp6n6U9B7grcDZKbm9SA0/C4WIiCcjYkdE7AQ+PcznNnv/dQJ/Anx5uD7N2n/1cOLI3A3MkTQ7/Vd6FrBkSJ8lQOUKltOB/zfcL06jpXOinwF+GhFXDdPndypjLpKOJvvejktik1SStHflPdkg6oNDui0B3p2urjoGeDp3Wma8DPufXjP3X07+Z+xc4JYqfZYBJ0nqTadiTkpthZM0D/gg8LaI2DJMn1p+FoqKLz9m9vZhPreW3/UinQg8HBFrqs1s5v6rS7NH5yfKi+yqn5+RXXHxodR2BdkvCcBUslMcK4EfAwePY2xvJDttcT+wPL3eAswH5qc+C4AVZFeJ3AX8wTjGd3D63PtSDJX9l49PwMK0fx8A5o7z97dElgj2ybU1bf+RJbAngEGy8+x/TjZm9h/Ao8B3gXLqOxf4t9yyf5Z+DlcC541jfCvJxgcqP4OVqwz3B5aO9LMwTvF9If1s3U+WDF42NL40/aLf9fGIL7XfUPmZy/Ud9/031pdLjpiZWV18qsrMzOrixGFmZnVx4jAzs7o4cZiZWV2cOMzMrC5OHDZhSPpB+jpL0rsavO7/Ve2ziiLptKIq7EraXNB6j5f0zTGu47GRbpyUtFjSnLF8hjWfE4dNGBHxB+ntLKCuxJHuyB3Jbokj91lF+SDwf8a6khq2q3ANjuEasn1jk5gTh00Yuf+kPwocm55H8DeSOtKzIO5OBezem/ofL+n7kpYAD6W2f0/F4VZUCsRJ+igwLa3vxvxnpTvZPy7pwfQMhDNz675d0teUPYPixtyd5R9V9myU+yX9S5XtOBR4PiJ+k6ZvkHStpAFJP5P01tRe83ZV+YyPpIKMd0l6ae5zTh+6P0fZlnmp7V6yUhiVZS+X9AVJ/wV8QVK/pJtSrHdL+m+pX5+k29L+/jeyGz0rd0B/K8X4YGW/At8HTpwICdHGoNl3IPrlV+UFbE5fjyf3bAzgfODS9H4vYACYnfo9C8zO9a3cbT2NrFRDX37dVT7rT8lKk3eQVaP9JdnzT44nq4A8k+wfrB+S3cHfBzwCu26e3bfKdpwHfCI3fQPw7bSeOWR3Ek+tZ7uGrD+A/57eX5lbxw3A6cPsz2rbMpXsTvA5ZH/wv1LZ72TPtrgHmJamF5EV3wM4iKz8DcDVpOdKAKek2PZL+/XTuVjyd+x/Bziq2T9vfu35y0ccNhmcRFbnajlZOfk+sj92AD+OiJ/n+l4oqVI25MBcv+G8EfhSZMXxngS+B7wut+41kRXNW052Cu1p4DngM5L+BKhWs+llwLohbV+JiJ2RldJeDbyyzu3K2wZUxiLuSXGNptq2vBL4eUQ8Gtlf9C8OWWZJRGxN708EPpViXQLMUFat+bjKchHxLWBj6v8A8GZJH5N0bEQ8nVvvU2RlNmyS8uGiTQYC3h8RuxXzk3Q82X/m+ekTgTdExBZJt5P9V72nns+930H29LvtyoognkBW7HIB8EdDlttKVj05b2htn6DG7apiMP2h3xVXer+ddPpZ0hSyJ9wNuy0jrL8iH8MU4JiIeG5IrFUXjIifKXs88FuAf5L0HxFxRZo9lWwf2STlIw6biJ4he0RuxTLgr5SVlkfSoaly6FD7ABtT0ngl2SNqKwYryw/xfeDMNN7QT/Yf9I+HCyz9l71PZKXX/wZ4bZVuPwVeMaTtHZKmSDqErJDdI3VsV60eA45K798GVNvevIeBWSkmyKoHD+c24P2VCUlHpLd3kC5kkHQy2eNskbQ/sCUivgh8nOwxqhWHMhErvlrNfMRhE9H9wI50yukG4JNkp1buTYO666j+WNVvA/Ml/ZTsD/NduXnXAfdLujcizs613wy8gawaaQAfjIhfp8RTzd7ALZKmkh0xfKBKnzuAT0hS7sjgl2QJaQZZddTn0mByLdtVq0+n2O4j2xcjHbWQYjgf+JakLWRJdO9hul8ILJR0P9nfjTvIqgv/I/AlSSuAH6TtBDgc+LiknWQVYv8KIA3kb42IX+/5ZlqzuTquWQEkfRL4RkR8V9INZIPOX2tyWE0n6W+ATRHxmWbHYnvOp6rMivHPQE+zg5iAfgt8rtlB2Nj4iMPMzOriIw4zM6uLE4eZmdXFicPMzOrixGFmZnVx4jAzs7r8f4h8RZWOrHzNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#\n",
        "#      EE 6183-Y01 Deep Learning  Homework #1  Group 2 Activation and Loss\n",
        "#       Import packages\n",
        "import h5py\n",
        "import scipy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "#\n",
        "# load_datasets\n",
        "#\n",
        "train_dataset = h5py.File('train_catvnoncat.h5',\"r\")\n",
        "train_set_x_orig=np.array(train_dataset[\"train_set_x\"][:])\n",
        "train_set_y_orig =np.array(train_dataset[\"train_set_y\"][:])\n",
        "\n",
        "test_dataset = h5py.File('test_catvnoncat.h5',\"r\")\n",
        "test_set_x_orig=np.array(test_dataset[\"test_set_x\"][:])\n",
        "test_set_y_orig =np.array(test_dataset[\"test_set_y\"][:])\n",
        "\n",
        "classes=np.array(test_dataset[\"list_classes\"][:])\n",
        "\n",
        "train_set_y_orig =train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "test_set_y_orig =test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "# \n",
        "# \n",
        "# Display dataset to check accessibility\n",
        "# \n",
        "\n",
        "# index = 25\n",
        "\n",
        "#plt.imshow(train_set_x_orig[index])\n",
        "#print (\"y =\" + str(train_set_y[:,index]) + \", it's a ' \" + \n",
        "#classes [np.squeeze(train_set_y[:,index])].decode(\"utf-8\") + \"'picture.\")\n",
        "\n",
        "\n",
        "#m_train = train_set_y.shape[1]\n",
        "#m_test = test_set_y.shape[1]\n",
        "#num_px = train_set_x_orig.shape[1]\n",
        "\n",
        "\n",
        "# We must reshape the training and test datasets into single vectors for input \n",
        "\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],\n",
        "-1).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], \n",
        "-1).T\n",
        "# print(\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "# print(\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "# print(\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "# print(\"test_set_y  shape: \" + str(test_set_y.shape))\n",
        "\n",
        "# now we Normalize the data by divide each row of data set by 255 \n",
        "\n",
        "train_set_x = train_set_x_flatten / 255.\n",
        "test_set_x = test_set_x_flatten / 255.\n",
        "\n",
        "# Y = train_set_y\n",
        "# X_Test = test_set_x\n",
        "# ---------------------------------------------------------------------------\n",
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "   Compute the sigmoid of z\n",
        "   Arguments: x -- A scalar od numpy array of any size\n",
        "   Return:\n",
        "   S -- sigmoid(z)\n",
        "   \"\"\"\n",
        "  s = 1 / (1 + np.exp(-z))\n",
        "  return s\n",
        "\n",
        "\n",
        "# print(\"sigmoid(0) =\" + str(sigmoid(0)))\n",
        "# print(\"sigmoid(9.2) = \" + str(sigmoid(9.2)))\n",
        "# -------------------------------------------------------------------------\n",
        "# dim -- size of the w vector we want\n",
        "# w -- initialized vector of shape (dim, 1)\n",
        "# b -- initialized scalar (corresponds to the bias)\n",
        "# Function creates a vector of zeroes of shape (dim, 1) for w and set b to 1\n",
        "# -------------------------------------------------------------------------\n",
        "def initialize_with_zeros(dim):\n",
        "  w = np.zeros(shape=(dim,1))\n",
        "  b = 1\n",
        "  return w,b\n",
        "# --------------------------------------------------------------------------\n",
        "# dim = 2\n",
        "# w, b = initialize_with_zeros(dim)\n",
        "# print(\"w = \" + str(w))\n",
        "# print(\"b = \" + str(b))\n",
        "\n",
        "def propagate(w,b,X,Y):\n",
        "  # Implement the loss function and its gradient for the forward and \n",
        "  # backward propagation\n",
        "  m = X.shape[1]\n",
        "  \n",
        "  # FORWARD PROPAGATION (GROUP 2)\n",
        "  # Activation function for Group 2 based on given function given\n",
        "  # ---------------------------------------------------------\n",
        "  A = sigmoid(np.dot(w.T,X) + (b))\n",
        "  loss = (1.0/(m)) * np.sum((Y-A)*(Y-A))\n",
        "  # ---------------------------------------------------------\n",
        "  # BACKWARD PROPAGATION (TO FIND GRADS)\n",
        "  Z1 = np.dot(A, (1-A).T)\n",
        "  Z2 = np.dot(X, (Y-A).T)\n",
        "  dw = (-1.0 / m) * (Z1*Z2)\n",
        "  db = (-1.0 / m) * np.sum(Z1*(Y-A))\n",
        "  assert(dw.shape == w.shape)\n",
        "  assert(db.dtype == float)\n",
        "  loss = np.squeeze(loss)\n",
        "  assert(loss.shape == ())\n",
        "\n",
        "  grads = {\"dw\": dw, \"db\": db}\n",
        "\n",
        "  return grads, loss\n",
        "#\n",
        "# -------------------------------------------------------------------------\n",
        "#  OPTIMIZE USING GRADIENT DESCENT\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "                                                                             \n",
        "                                                                             \n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_loss = False):\n",
        "  # This function optimizes w and b by running a gradient_descent algorithm\n",
        "  # w -- weights, a nump array of size (num_px * num_px *)\n",
        "  # b -- bias. a scalar\n",
        "  # X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "  # Y -- true \"label\" vector (0 if non-cat, 1 if cat)\n",
        "  # num_iterations \n",
        "  # learning_rate -- learning rate of the gradient descent update rule\n",
        "  # print_loss -- True means to print the loss every 100 steps\n",
        "  #\n",
        "  # Returns\n",
        "  # params -- dictionary containing the weiths w and bias b \n",
        "  # grads -- dictionary containing the gradients of the weights and bias with\n",
        "  #          respect to the loss function\n",
        "  # Loss -- List of all the loss computed during the optimization, this will\n",
        "  #         be used to plot the learning curve\n",
        "  losses = []\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "    # Loss and gradient calculation (~ 1-4 lines of code)\n",
        "    grads, loss = propagate(w, b, X, Y)\n",
        "    # Retrieve derivatives from grads\n",
        "    dw = grads[\"dw\"]\n",
        "    db = grads[\"db\"]\n",
        "    # update rule (~ 2 lines of code)\n",
        "    w = w - learning_rate * dw \n",
        "    b = b -learning_rate * db\n",
        "    # Record the losses\n",
        "    if i % 100 == 0:\n",
        "      losses.append(loss)\n",
        "    # Print the loss every 100 training examples\n",
        "    if print_loss and i % 100 == 0:\n",
        "      print(\"Loss after iteration %i: %f\" % (i, loss))\n",
        "  params = {\"w\":w, \"b\": b}\n",
        "  grads = {\"dw\": dw, \"db\": db}\n",
        "\n",
        "  return params, grads, losses\n",
        "#\n",
        "#\n",
        "# Predict --------------------------------------------------------------------\n",
        "#\n",
        "def predict(w, b, X):\n",
        "  # Predict whether the label is 0 or 1 using learned Logistics \n",
        "  # regression parameters (w,b)\n",
        "  # \n",
        "  # Returns: \n",
        "  #    Y_prediction -- a numpy array (vector) containing all predictiions\n",
        "  #                    (0/1) for the examples in X\n",
        "  #\n",
        "  m = X.shape[1]\n",
        "  Y_prediction = np.zeros((1,m))\n",
        "  w = w.reshape(X.shape[0],1)\n",
        "  # Compute vector \"A\" predicting the probabilities of a cat being present\n",
        "  # in the picture \n",
        "  #\n",
        "  A = sigmoid(np.dot(w.T,X) + b)\n",
        "  for i in range(A.shape[1]):\n",
        "    # Convert probabilities a [0,i] to actual predictions p[0,1]\n",
        "    Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
        "\n",
        "    assert(Y_prediction.shape == (1,m))\n",
        "\n",
        "    return Y_prediction\n",
        "\n",
        "### -------------------------------------------------------------------------\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate =\n",
        ".01, print_loss=False):\n",
        "#\n",
        "# initialize parameters with zeros ()\n",
        "#\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "# Gradient descent \n",
        "    params, grads, losses = optimize(w,b, X_train, Y_train,num_iterations,\n",
        "    learning_rate, print_loss)\n",
        "    w = params[\"w\"]\n",
        "    b = params[\"b\"]\n",
        "# Predict test/test/train set examples\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "#\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train\n",
        "    - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100-np.mean(np.abs(Y_prediction_test\n",
        "    - Y_test )) * 100))\n",
        "\n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\" : Y_prediction_train,\n",
        "         \"w\" : w,\n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    return d\n",
        "# --------------------------------------------------------------------------\n",
        "# Run the model \n",
        "# --------------------------------------------------------------------------\n",
        "d = model(train_set_x, train_set_y_orig, test_set_x, test_set_y_orig, num_iterations = \n",
        "          2000, learning_rate=.01, print_loss = True)\n",
        "# --------------------------------------------------------------------------\n",
        "# Plot the Results\n",
        "losses = np.squeeze(d['losses'])\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss - Group 2')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GB1HhvnS_Gyp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PP-BAJhU4_7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WZffmfKD5EY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mCHVivVBmHlM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C5UJv5bBlvGJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8H0b5U2Hlv2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "KTblfELzj348"
      }
    }
  ]
}